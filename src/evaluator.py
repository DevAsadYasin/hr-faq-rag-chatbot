import json
import logging
import sys
from typing import Dict, List, Any

from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

from llm_manager import LLMManager

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('outputs/logs/evaluator.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class EvaluationResult(BaseModel):
    score: int = Field(
        description="Overall quality score from 0-10",
        ge=0,
        le=10
    )
    reason: str = Field(
        description="Detailed reasoning for the score"
    )
    chunk_relevance_score: float = Field(
        description="Relevance of retrieved chunks to the question (0-1)",
        ge=0.0,
        le=1.0
    )
    answer_accuracy_score: float = Field(
        description="Accuracy of the answer based on chunks (0-1)",
        ge=0.0,
        le=1.0
    )
    completeness_score: float = Field(
        description="Completeness of the answer (0-1)",
        ge=0.0,
        le=1.0
    )


def create_evaluation_chain():
    parser = PydanticOutputParser(pydantic_object=EvaluationResult)
    system_template = """You are an expert evaluator for RAG (Retrieval-Augmented Generation) systems.
Your task is to evaluate the quality of answers generated by a FAQ support chatbot.

Evaluate based on three criteria:
1. Chunk Relevance (0-1): How relevant are the retrieved chunks to the user's question?
   - 1.0: Chunks directly address the question
   - 0.5: Chunks are somewhat related but not directly relevant
   - 0.0: Chunks are not relevant to the question

2. Answer Accuracy (0-1): Is the answer accurate based on the provided chunks?
   - 1.0: Answer is completely accurate and supported by chunks
   - 0.5: Answer is partially accurate or has minor inaccuracies
   - 0.0: Answer is incorrect or contradicts the chunks

3. Completeness (0-1): Does the answer fully address the question?
   - 1.0: Answer fully addresses all aspects of the question
   - 0.5: Answer addresses main points but misses some details
   - 0.0: Answer is incomplete or doesn't address the question

Overall Score (0-10): Weighted average
- 10: Excellent (all criteria > 0.9)
- 7-9: Good (all criteria > 0.7)
- 4-6: Fair (criteria > 0.5)
- 0-3: Poor (any criterion < 0.5)

{format_instructions}"""

    system_prompt = SystemMessagePromptTemplate.from_template(system_template)
    human_template = """Question: {question}

Answer: {answer}

Retrieved Chunks ({num_chunks} chunks):
{chunks}

Evaluate this response and provide scores with detailed reasoning."""

    human_prompt = HumanMessagePromptTemplate.from_template(human_template)
    chat_prompt = ChatPromptTemplate.from_messages([
        system_prompt,
        human_prompt
    ])
    llm_manager = LLMManager()
    evaluator_llm = llm_manager.get_llm()
    
    if not evaluator_llm:
        raise ValueError(
            "No LLM available for evaluation. Please set at least one API key: "
            "OPENROUTER_API_KEY, GEMINI_API_KEY, or OPENAI_API_KEY"
        )
    chain = chat_prompt | evaluator_llm | parser
    
    return chain, parser


def evaluate_response(
    question: str,
    answer: str,
    chunks: List[Dict[str, Any]]
) -> Dict[str, Any]:
    logger.info(f"Evaluating response for question: {question[:50]}...")
    
    try:
        chain, parser = create_evaluation_chain()
        chunks_text = "\n\n".join([
            f"Chunk {i+1} (ID: {chunk.get('chunk_id', i)}):\n{chunk.get('chunk_text', '')}"
            for i, chunk in enumerate(chunks)
        ])
        result = chain.invoke({
            "question": question,
            "answer": answer,
            "chunks": chunks_text,
            "num_chunks": len(chunks),
            "format_instructions": parser.get_format_instructions()
        })
        evaluation = result.dict()
        logger.info(f"Evaluation completed. Score: {evaluation['score']}/10")
        return evaluation
        
    except Exception as e:
        logger.error(f"Error during evaluation: {str(e)}")
        raise


def main():
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
        logger.info(f"Loading response from: {input_file}")
        with open(input_file, "r") as f:
            data = json.load(f)
        if isinstance(data, list):
            responses = data
        else:
            responses = [data]
    else:
        logger.info("No input file provided, using sample data")
        responses = [{
            "user_question": "Where is the authentication service deployed in production?",
            "system_answer": "The authentication service is deployed in production across multiple regions: 30 replicas in us-east-1, 12 replicas in eu-west-1, and 8 replicas in ap-southeast-1.",
            "chunks_related": [
                {
                    "chunk_id": 0,
                    "chunk_text": "Authentication Service - Replicas: 30 (us-east-1), 12 (eu-west-1), 8 (ap-southeast-1)",
                    "metadata": {}
                }
            ]
        }]
    
    logger.info("=" * 60)
    logger.info("RAG Answer Evaluator")
    logger.info("=" * 60)
    
    evaluations = []
    
    for response in responses:
        try:
            evaluation = evaluate_response(
                question=response["user_question"],
                answer=response["system_answer"],
                chunks=response["chunks_related"]
            )
            evaluation["user_question"] = response["user_question"]
            evaluation["system_answer"] = response["system_answer"]
            
            evaluations.append(evaluation)
            print("\n" + "=" * 60)
            print("EVALUATION RESULT:")
            print("=" * 60)
            print(json.dumps(evaluation, indent=2, ensure_ascii=False))
            
        except Exception as e:
            logger.error(f"Evaluation failed: {str(e)}")
            continue
    
    if evaluations:
        output_file = "outputs/evaluations.json"
        with open(output_file, "w") as f:
            json.dump(evaluations, f, indent=2, ensure_ascii=False)
        logger.info(f"\nEvaluations saved to: {output_file}")


if __name__ == "__main__":
    main()

